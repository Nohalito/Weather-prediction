{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jOIngx2Cme6"
   },
   "source": [
    "# Predictive Methods Project :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZbzFHW_CtLC"
   },
   "source": [
    "<ul>\n",
    "  <li></li>\n",
    "  <li></li>\n",
    "  <li></li>\n",
    "  <li></li>\n",
    "  <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mQHtqHAGh7M"
   },
   "source": [
    "Rendu :<br>\n",
    "- Notebook =>  <br>\n",
    "- Rapport =>  <br>\n",
    "- Brouillon, plan, idées & liens utiles =>  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV0T6CgWDWSF"
   },
   "source": [
    "## 0°/ Introduction :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSocdZiKEo_Y"
   },
   "source": [
    "This part will be filled when we find a correct source of data and all agreed on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42l0wrk1DYx-"
   },
   "source": [
    "## 1°/ Set up :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBw-KOxwDckb"
   },
   "source": [
    "### 1.1°/ Pip install & Import :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGLUSUsxFv-I"
   },
   "outputs": [],
   "source": [
    "# Some libraries may need to be installed beforehand, add any !pip install here\n",
    "#!pip install pandas\n",
    "#!pip install geopandas\n",
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7TKapxzpFvjD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Useful to show loop progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Turn off warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em-zGaURFGxq"
   },
   "source": [
    "### 1.2°/ Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# == I set up the Corse zone (2A, 2B) as number (201, 202)   ==\n",
    "# == Be mindful of this particularity throughout the project ==\n",
    "# =============================================================\n",
    "\n",
    "dict_test_2_first_dept = {\n",
    "    '01' : \"Ain\", \n",
    "    '02' : \"Aisne\"\n",
    "}\n",
    "\n",
    "En_France_métropolitaine = {\n",
    "    '01' : \"Ain\", \n",
    "    '02' : \"Aisne\", \n",
    "    '03' : \"Allier\", \n",
    "    '04' : \"Alpes-de-Haute-Provence\", \n",
    "    '05' : \"Hautes-Alpes\", \n",
    "    '06' : \"Alpes-Maritimes\", \n",
    "    '07' : \"Ardèche\", \n",
    "    '08' : \"Ardennes\", \n",
    "    '09' : \"Ariège\", \n",
    "    '10' : \"Aube\", \n",
    "    '11' : \"Aude\", \n",
    "    '12' : \"Aveyron\", \n",
    "    '13' : \"Bouches-du-Rhône\", \n",
    "    '14' : \"Calvados\", \n",
    "    '15' : \"Cantal\", \n",
    "    '16' : \"Charente\", \n",
    "    '17' : \"Charente-Maritime\", \n",
    "    '18' : \"Cher\", \n",
    "    '19' : \"Corrèze\", \n",
    "    '201' : \"Corse-du-Sud\", \n",
    "    '202' : \"Haute-Corse\", \n",
    "    '21' : \"Côte-d'Or\", \n",
    "    '22' : \"Côtes-d'Armor\", \n",
    "    '23' : \"Creuse\", \n",
    "    '24' : \"Dordogne\", \n",
    "    '25' : \"Doubs\", \n",
    "    '26' : \"Drôme\", \n",
    "    '27' : \"Eure\", \n",
    "    '28' : \"Eure-et-Loir\", \n",
    "    '29' : \"Finistère\", \n",
    "    '30' : \"Gard\", \n",
    "    '31' : \"Haute-Garonne\", \n",
    "    '32' : \"Gers\", \n",
    "    '33' : \"Gironde\", \n",
    "    '34' : \"Hérault\", \n",
    "    '35' : \"Ille-et-Vilaine\", \n",
    "    '36' : \"Indre\", \n",
    "    '37' : \"Indre-et-Loire\", \n",
    "    '38' : \"Isère\", \n",
    "    '39' : \"Jura\", \n",
    "    '40' : \"Landes\", \n",
    "    '41' : \"Loir-et-Cher\", \n",
    "    '42' : \"Loire\", \n",
    "    '43' : \"Haute-Loire\", \n",
    "    '44' : \"Loire-Atlantique\", \n",
    "    '45' : \"Loiret\", \n",
    "    '46' : \"Lot\", \n",
    "    '47' : \"Lot-et-Garonne\", \n",
    "    '48' : \"Lozère\", \n",
    "    '49' : \"Maine-et-Loire\", \n",
    "    '50' : \"Manche\", \n",
    "    '51' : \"Marne\", \n",
    "    '52' : \"Haute-Marne\", \n",
    "    '53' : \"Mayenne\", \n",
    "    '54' : \"Meurthe-et-Moselle\", \n",
    "    '55' : \"Meuse\", \n",
    "    '56' : \"Morbihan\", \n",
    "    '57' : \"Moselle\", \n",
    "    '58' : \"Nièvre\", \n",
    "    '59' : \"Nord\", \n",
    "    '60' : \"Oise\", \n",
    "    '61' : \"Orne\", \n",
    "    '62' : \"Pas-de-Calais\", \n",
    "    '63' : \"Puy-de-Dôme\", \n",
    "    '64' : \"Pyrénées-Atlantiques\", \n",
    "    '65' : \"Hautes-Pyrénées\", \n",
    "    '66' : \"Pyrénées-Orientales\", \n",
    "    '67' : \"Bas-Rhin\", \n",
    "    '68' : \"Haut-Rhin\", \n",
    "    '69' : \"Rhône\", \n",
    "    '70' : \"Haute-Saône\", \n",
    "    '71' : \"Saône-et-Loire\", \n",
    "    '72' : \"Sarthe\", \n",
    "    '73' : \"Savoie\", \n",
    "    '74' : \"Haute-Savoie\", \n",
    "    '75' : \"Paris\", \n",
    "    '76' : \"Seine-Maritime\", \n",
    "    '77' : \"Seine-et-Marne\", \n",
    "    '78' : \"Yvelines\", \n",
    "    '79' : \"Deux-Sèvres\", \n",
    "    '80' : \"Somme\", \n",
    "    '81' : \"Tarn\", \n",
    "    '82' : \"Tarn-et-Garonne\", \n",
    "    '83' : \"Var\", \n",
    "    '84' : \"Vaucluse\", \n",
    "    '85' : \"Vendée\", \n",
    "    '86' : \"Vienne\", \n",
    "    '87' : \"Haute-Vienne\", \n",
    "    '88' : \"Vosges\", \n",
    "    '89' : \"Yonne\", \n",
    "    '90' : \"Territoire_de_Belfort\", \n",
    "    '91' : \"Essonne\", \n",
    "    '92' : \"Hauts-de-Seine\", \n",
    "    '93' : \"Seine-Saint-Denis\", \n",
    "    '94' : \"Val-de-Marne\", \n",
    "    '95' : \"Val-d'Oise\"\n",
    "}\n",
    "\n",
    "# Well kinda useless since the region is different\n",
    "En_outre_mer = {\n",
    "    '971' : \"Guadeloupe\",\n",
    "    '972' : \"Martinique\",\n",
    "    '973' : \"Guyane\",\n",
    "    '974' : \"La-Réunion\",\n",
    "    '976' : \"Mayotte\"\n",
    "}\n",
    "\n",
    "Region = {\n",
    "    \"Auvergne-Rhône-Alpes\": [1, 3, 7, 15, 26, 38, 42, 43, 69, 73, 74], # And this\n",
    "    \"Bourgogne-Franche-Comté\": [21, 25, 39, 58, 70, 71, 89, 90],\n",
    "    \"Bretagne\": [22, 29, 35, 56],\n",
    "    \"Centre-Val de Loire\": [18, 28, 36, 37, 41, 45],\n",
    "    \"Grand Est\": [8, 10, 51, 52, 54, 55, 57, 67, 68, 88],\n",
    "    \"Hauts-de-France\": [2, 59, 60, 62, 80],\n",
    "    \"Île-de-France\": [75, 77, 78, 91, 92, 93, 94, 95],\n",
    "    \"Normandie\": [14, 27, 50, 61, 76],\n",
    "    \"Nouvelle-Aquitaine\": [16, 17, 19, 23, 24, 33, 40, 47, 64, 79, 86, 87],\n",
    "    \"Occitanie\": [9, 11, 12, 30, 31, 32, 34, 46, 48, 65, 66, 81, 82],\n",
    "    \"Pays de la Loire\": [44, 49, 53, 72, 85],\n",
    "    \"Provence-Alpes-Côte d'Azur\": [4, 6, 13, 83, 84] # This\n",
    "}\n",
    "\n",
    "# df.columns => ctrl + c / ctrl + v\n",
    "Col_RTT = ['NUM_POSTE', 'NOM_USUEL', 'LAT', 'LON', 'ALTI', 'AAAAMMJJ', 'RR', 'QRR',\n",
    "           'TN', 'QTN', 'HTN', 'QHTN', 'TX', 'QTX', 'HTX', 'QHTX', 'TM', 'QTM',\n",
    "           'TNTXM', 'QTNTXM', 'TAMPLI', 'QTAMPLI', 'TNSOL', 'QTNSOL', 'TN50',\n",
    "           'QTN50', 'DG', 'QDG', 'FFM', 'QFFM', 'FF2M', 'QFF2M', 'FXY', 'QFXY',\n",
    "           'DXY', 'QDXY', 'HXY', 'QHXY', 'FXI', 'QFXI', 'DXI', 'QDXI', 'HXI',\n",
    "           'QHXI', 'FXI2', 'QFXI2', 'DXI2', 'QDXI2', 'HXI2', 'QHXI2', 'FXI3S',\n",
    "           'QFXI3S', 'DXI3S', 'QDXI3S', 'HXI3S', 'QHXI3S', 'DRR', 'QDRR']\n",
    "\n",
    "# df.columns => ctrl + c / ctrl + v\n",
    "Col_autre_para = ['NUM_POSTE', 'NOM_USUEL', 'LAT', 'LON', 'ALTI', 'AAAAMMJJ', 'DHUMEC',\n",
    "                  'QDHUMEC', 'PMERM', 'QPMERM', 'PMERMIN', 'QPMERMIN', 'INST', 'QINST',\n",
    "                  'GLOT', 'QGLOT', 'DIFT', 'QDIFT', 'DIRT', 'QDIRT', 'INFRART',\n",
    "                  'QINFRART', 'UV', 'QUV', 'UV_INDICEX', 'QUV_INDICEX', 'SIGMA', 'QSIGMA',\n",
    "                  'UN', 'QUN', 'HUN', 'QHUN', 'UX', 'QUX', 'HUX', 'QHUX', 'UM', 'QUM',\n",
    "                  'DHUMI40', 'QDHUMI40', 'DHUMI80', 'QDHUMI80', 'TSVM', 'QTSVM', 'ETPMON',\n",
    "                  'QETPMON', 'ETPGRILLE', 'QETPGRILLE', 'ECOULEMENTM', 'QECOULEMENTM',\n",
    "                  'HNEIGEF', 'QHNEIGEF', 'NEIGETOTX', 'QNEIGETOTX', 'NEIGETOT06',\n",
    "                  'QNEIGETOT06', 'NEIG', 'QNEIG', 'BROU', 'QBROU', 'ORAG', 'QORAG',\n",
    "                  'GRESIL', 'QGRESIL', 'GRELE', 'QGRELE', 'ROSEE', 'QROSEE', 'VERGLAS',\n",
    "                  'QVERGLAS', 'SOLNEIGE', 'QSOLNEIGE', 'GELEE', 'QGELEE', 'FUMEE',\n",
    "                  'QFUMEE', 'BRUME', 'QBRUME', 'ECLAIR', 'QECLAIR', 'NB300', 'QNB300',\n",
    "                  'BA300', 'QBA300', 'TMERMIN', 'QTMERMIN', 'TMERMAX', 'QTMERMAX',\n",
    "                  'Year']\n",
    "\n",
    "Col_to_drop = ['FFM', 'FF2M', 'FXY', 'DXY', 'HXY', 'FXI', 'DXI', 'HXI', 'FXI2', 'DXI2',\n",
    "               'HXI2', 'FXI3S', 'DXI3S', 'HXI3S', 'PMERM', 'PMERMIN', 'GLOT', 'DIFT',\n",
    "               'INFRART', 'UV_INDICEX', 'SIGMA', 'UN', 'HUN', 'UX', 'HUX', 'UM', 'DHUMI40',\n",
    "               'DHUMI80', 'TSVM', 'ETPMON', 'ETPGRILLE', 'ECOULEMENTM', 'HNEIGEF', 'NEIGETOTX',\n",
    "               'NEIGETOT06', 'NB300', 'BA300', 'TMERMIN', 'TMERMAX']\n",
    "\n",
    "Q_col_to_drop = ['QRR','QTN', 'QHTN', 'QTX', 'QHTX', 'QTM', 'QTNTXM', 'QTAMPLI', 'QTNSOL',\n",
    "                 'QTN50', 'QDG', 'QFFM', 'QFF2M', 'QFXY', 'QDXY', 'QHXY', 'QFXI', 'QDXI',\n",
    "                 'QHXI', 'QFXI2', 'QDXI2', 'QHXI2', 'QFXI3S', 'QDXI3S', 'QHXI3S', 'QDRR',\n",
    "                 'QDHUMEC', 'QPMERM', 'QPMERMIN', 'QINST', 'QGLOT', 'QDIFT', 'QDIRT',\n",
    "                 'QINFRART', 'QUV', 'QUV_INDICEX', 'QSIGMA', 'QUN', 'QHUN', 'QUX', 'QHUX', 'QUM',\n",
    "                 'QDHUMI40', 'QDHUMI80', 'QTSVM', 'QETPMON', 'QETPGRILLE', 'QECOULEMENTM',\n",
    "                 'QHNEIGEF', 'QNEIGETOTX', 'QNEIGETOT06', 'QNEIG', 'QBROU', 'QORAG', 'QGRESIL',\n",
    "                 'QGRELE', 'QROSEE', 'QVERGLAS', 'QSOLNEIGE', 'QGELEE', 'QFUMEE', 'QBRUME',\n",
    "                 'QECLAIR', 'QNB300', 'QBA300', 'QTMERMIN', 'QTMERMAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_data_cleaning(df, start_year, end_year):\n",
    "    \"\"\"\n",
    "    This function does 2 things :\n",
    "    - By manually setting it, keep only data between 2 given year\n",
    "    - It allow to drop all rows with NaN value that should have contained mandatory information\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm a baka\n",
    "    df['AAAAMMJJ'] = df['AAAAMMJJ'].astype('str')\n",
    "    df['NUM_POSTE'] = df['NUM_POSTE'].astype('str')\n",
    "\n",
    "    # Drop rows without id\n",
    "    df = df.dropna(subset = ['NUM_POSTE', 'AAAAMMJJ'])\n",
    "\n",
    "    # ===========================================\n",
    "    # == Only keep Data between the set period ==\n",
    "    # ===========================================\n",
    "    df = df[df['AAAAMMJJ'].str[:4].astype('int').between(start_year, end_year)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def modify_columns(df, key, value):\n",
    "    \"\"\"\n",
    "    Extract the feature of the date columns\n",
    "    Also assign the department number and name to the extracted data\n",
    "    \"\"\"\n",
    "    df = df.drop(Col_to_drop, axis = 1)\n",
    "    df = df.drop(Q_col_to_drop, axis = 1)\n",
    "\n",
    "    df['Year'] = df['AAAAMMJJ'].str[:4]\n",
    "    df['Month'] = df['AAAAMMJJ'].str[4:6]\n",
    "    df['Day'] = df['AAAAMMJJ'].str[-2:]\n",
    "    \n",
    "    df['Dept_numb'] = key\n",
    "    df['Dept_name'] = value\n",
    "\n",
    "    return df\n",
    "\n",
    "def webo_scrappo_el_dataframo(start_y, end_y):\n",
    "    \"\"\"\n",
    "    Extract the weather data from all department of the metropolitan France on a given period of time\n",
    "    This function use the following link :\n",
    "    https://www.data.gouv.fr/datasets/donnees-climatologiques-de-base-quotidiennes\n",
    "    To extract from data.gouv for each department all 4 related datasets.\n",
    "    Then proceed by merging and concatenating all of them\n",
    "    \"\"\"\n",
    "    df_metropo = pd.DataFrame()\n",
    "    dept_error = []\n",
    "\n",
    "    # Data source : https://www.data.gouv.fr/datasets/donnees-climatologiques-de-base-quotidiennes\n",
    "\n",
    "    # The download link is such as :\n",
    "    # url_base + \"department number\" + url_end{RTT_1950, autre_1950, RTT_2025, autre_2025}\n",
    "    # We have 95 department in the metropolitan area : 95 'Dept_number'\n",
    "    # All department have a dataset for RTT & Other_feature, then those cover both the 1950-2023 period and 2024-2025 period : 4 datasets per department\n",
    "    # => around 95*4 = 380 url\n",
    "\n",
    "    url_base = \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_\"\n",
    "    \n",
    "    url_end_RTT_1950 = \"_previous-1950-2023_RR-T-Vent.csv.gz\"\n",
    "    url_end_autre_1950 = \"_previous-1950-2023_autres-parametres.csv.gz\"\n",
    "    url_end_RTT_2025 = \"_latest-2024-2025_RR-T-Vent.csv.gz\"\n",
    "    url_end_autre_2025 = \"_latest-2024-2025_autres-parametres.csv.gz\"\n",
    "\n",
    "    for key, value in tqdm(dict_test_2_first_dept.items(), desc=\"Downloading depts\"):\n",
    "\n",
    "        try :\n",
    "\n",
    "            url = url_base + key\n",
    "            df_temp_RTT_1950 = pd.read_csv(url + url_end_RTT_1950, sep = ';', compression = 'gzip')\n",
    "            df_temp_autre_1950 = pd.read_csv(url + url_end_autre_1950, sep = ';', compression = 'gzip')\n",
    "            df_temp_RTT_2025 = pd.read_csv(url + url_end_RTT_2025, sep = ';', compression = 'gzip')\n",
    "            df_temp_autre_2025 = pd.read_csv(url + url_end_autre_2025, sep = ';', compression = 'gzip')\n",
    "\n",
    "            df_temp_RTT_1950 = base_data_cleaning(df_temp_RTT_1950, start_y, end_y)\n",
    "            df_temp_autre_1950 = base_data_cleaning(df_temp_autre_1950, start_y, end_y)\n",
    "            df_temp_RTT_2025 = base_data_cleaning(df_temp_RTT_2025, start_y, end_y)\n",
    "            df_temp_autre_2025 = base_data_cleaning(df_temp_autre_2025, start_y, end_y)\n",
    "\n",
    "            df_temp_1950 = pd.merge(df_temp_RTT_1950, df_temp_autre_1950, on = ['NUM_POSTE', 'NOM_USUEL', 'LAT', 'LON', 'ALTI', 'AAAAMMJJ'], how = \"left\")\n",
    "            df_temp_2025 = pd.merge(df_temp_RTT_2025, df_temp_autre_2025, on = ['NUM_POSTE', 'NOM_USUEL', 'LAT', 'LON', 'ALTI', 'AAAAMMJJ'], how = \"left\")\n",
    "\n",
    "            df_temp = pd.concat([df_temp_1950, df_temp_2025])\n",
    "            \n",
    "            df_temp = modify_columns(df_temp, key, value)\n",
    "\n",
    "            df_metropo = pd.concat([df_metropo, df_temp])\n",
    "            \n",
    "            #print(key)\n",
    "        \n",
    "        except:\n",
    "            dept_error.append(key)\n",
    "\n",
    "        # This print the progress\n",
    "        # tqdm.write(f\"➡ Dept: {key}\")\n",
    "\n",
    "    print(dept_error)\n",
    "\n",
    "    return df_metropo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading depts: 100%|██████████| 2/2 [00:55<00:00, 28.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(191348, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = webo_scrappo_el_dataframo(2015, 2025)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For when the dataframe will be clean :\n",
    "#df.to_csv(\"datasets/Weather_data_2010-2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM_POSTE</th>\n",
       "      <th>NOM_USUEL</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>ALTI</th>\n",
       "      <th>AAAAMMJJ</th>\n",
       "      <th>RR</th>\n",
       "      <th>QRR</th>\n",
       "      <th>TN</th>\n",
       "      <th>QTN</th>\n",
       "      <th>...</th>\n",
       "      <th>HXI2</th>\n",
       "      <th>QHXI2</th>\n",
       "      <th>FXI3S</th>\n",
       "      <th>QFXI3S</th>\n",
       "      <th>DXI3S</th>\n",
       "      <th>QDXI3S</th>\n",
       "      <th>HXI3S</th>\n",
       "      <th>QHXI3S</th>\n",
       "      <th>DRR</th>\n",
       "      <th>QDRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1014002</td>\n",
       "      <td>ARBENT</td>\n",
       "      <td>46.278167</td>\n",
       "      <td>5.669</td>\n",
       "      <td>534</td>\n",
       "      <td>20240101</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1014002</td>\n",
       "      <td>ARBENT</td>\n",
       "      <td>46.278167</td>\n",
       "      <td>5.669</td>\n",
       "      <td>534</td>\n",
       "      <td>20240102</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1014002</td>\n",
       "      <td>ARBENT</td>\n",
       "      <td>46.278167</td>\n",
       "      <td>5.669</td>\n",
       "      <td>534</td>\n",
       "      <td>20240103</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1014002</td>\n",
       "      <td>ARBENT</td>\n",
       "      <td>46.278167</td>\n",
       "      <td>5.669</td>\n",
       "      <td>534</td>\n",
       "      <td>20240104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1014002</td>\n",
       "      <td>ARBENT</td>\n",
       "      <td>46.278167</td>\n",
       "      <td>5.669</td>\n",
       "      <td>534</td>\n",
       "      <td>20240105</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NUM_POSTE NOM_USUEL        LAT    LON  ALTI  AAAAMMJJ    RR  QRR   TN  QTN  \\\n",
       "0    1014002    ARBENT  46.278167  5.669   534  20240101   3.2  1.0  1.8  1.0   \n",
       "1    1014002    ARBENT  46.278167  5.669   534  20240102  15.2  1.0  5.0  1.0   \n",
       "2    1014002    ARBENT  46.278167  5.669   534  20240103   8.0  1.0  7.5  1.0   \n",
       "3    1014002    ARBENT  46.278167  5.669   534  20240104   0.0  1.0  2.8  1.0   \n",
       "4    1014002    ARBENT  46.278167  5.669   534  20240105   8.4  1.0  1.8  1.0   \n",
       "\n",
       "   ...  HXI2  QHXI2  FXI3S  QFXI3S  DXI3S  QDXI3S   HXI3S  QHXI3S  DRR  QDRR  \n",
       "0  ...   NaN    NaN    8.5     1.0  140.0     1.0  2259.0     9.0  NaN   NaN  \n",
       "1  ...   NaN    NaN   17.6     1.0  140.0     1.0  1930.0     9.0  NaN   NaN  \n",
       "2  ...   NaN    NaN   14.2     1.0  210.0     1.0  1648.0     9.0  NaN   NaN  \n",
       "3  ...   NaN    NaN   11.1     1.0  150.0     1.0  2203.0     9.0  NaN   NaN  \n",
       "4  ...   NaN    NaN    8.2     1.0  180.0     1.0    26.0     9.0  NaN   NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 1950-2023 RRT\n",
    "# \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_01_previous-1950-2023_RR-T-Vent.csv.gz\"\n",
    "# # 1950-2023 Autre para\n",
    "# \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_01_previous-1950-2023_autres-parametres.csv.gz\"\n",
    "# # 2024-2025 RRT\n",
    "# \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_01_latest-2024-2025_RR-T-Vent.csv.gz\"\n",
    "# # 2024-2025 Autre para\n",
    "# \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_01_latest-2024-2025_autres-parametres.csv.gz\"\n",
    "# \n",
    "# df_test = pd.read_csv(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_01_latest-2024-2025_RR-T-Vent.csv.gz\", sep = ';',compression = 'gzip')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3°/ Visualization set up :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're working on an anaconda environment, do the following :\n",
    "# *Open anaconda prompt terminal* (NOT ANOTHER ONE, THIS ONE ESPECIALLY)\n",
    "# conda create -n \"name_of_your_environment\" python=3.14\n",
    "# conda activate name_of_your_environment\n",
    "# conda install -c conda-forge geopandas\n",
    "# pip install of conda install on any other terminal will end in a error with the \"pyogrio\" module. Why ? The hell I don't know.\n",
    "\n",
    "# https://www.data.gouv.fr/api/1/datasets/r/aacf9338-8944-4513-a7b9-4cd7c2db2fa9\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\anaconda\\envs\\Predictive_methods\\python.exe\n",
      "Matplotlib version: 3.10.7\n",
      "Backend: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "Python: c:\\anaconda\\envs\\Predictive_methods\\python.exe\n",
    "Matplotlib version: 3.10.7\n",
    "Backend: module://matplotlib_inline.backend_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non metropolitan France + Corsia\n",
    "reg_useless = ['01', '02', '03', '04', '06', '94']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\Predictive_methods\\Lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files. Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('datasets/regions-20180101.shp')\n",
    "gdf = gdf[~gdf['code_insee'].isin(reg_useless)]\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001VR5-fDkjX"
   },
   "source": [
    "## 2°/ Pre-processing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR7oLrnsE4Bp"
   },
   "source": [
    "### 2.0°/ Merge datasets (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxXgmi0mD9xd"
   },
   "source": [
    "### 2.1 : Data cleaning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3jiRFL5GAw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yS3AhlPWDrJg"
   },
   "source": [
    "### 2.2°/ Explanatory data analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nug-YddDGCoc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVmV_-QYDztu"
   },
   "source": [
    "## 3°/ Feature engineering (if needed) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKGslKd0EKjb"
   },
   "source": [
    "<ul>\n",
    "  <li>List here the different feature extraction we can use depending on the model we will use. (Again this part may not be used so don't waste time on it)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoWQQ5LJEdR5"
   },
   "source": [
    "## 4°/ Modeling :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR318Y6GFdsm"
   },
   "source": [
    "We can do multiple model here to obtain different results so that we can debate over them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "993OOFyrFV_4"
   },
   "source": [
    "### 4.1°/ 1st model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDo80AvsGDfI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEqEYm2eFVqy"
   },
   "source": [
    "### 4.i°/ i-ième model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yuc3OpJEHg2M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3X3DNkgMEh3H"
   },
   "source": [
    "## 5°/ Models prediction & Metrics evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OlnnNHuGEKB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Predictive_methods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
